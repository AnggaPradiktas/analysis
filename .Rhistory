videos = yt_search(term = "", type = "video", channel_id = "UCo8h2TY_uBkAVUIc14m_KCA")
videos = videos %<%
library(tuber)
videos = yt_search(term = "", type = "video", channel_id = "UCo8h2TY_uBkAVUIc14m_KCA")
ID = "889738434646-fvd4a9d45u88g4kbmdhprhslqqbiq9nt.apps.googleusercontent.com"
PASS = "inzpIq33HttfcKCbRYvr48XE"
scopes = "https://www.googleapis.com/auth/youtube.readonly"
yt_oauth(ID,
PASS, token = "")
chstat = get_channel_stats("UCo8h2TY_uBkAVUIc14m_KCA")
videos = yt_search(term = "", type = "video", channel_id = "UCo8h2TY_uBkAVUIc14m_KCA")
View(videos)
videos = videos %<%
mutate(date = as.Date(publishedAt)) %>%
filter(date > "2018-01-01") %>%
arrange(date)
library("dbplyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library("dplyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
library("tidytext", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
videos = videos %<%
mutate(date = as.Date(publishedAt)) %>%
filter(date > "2018-01-01") %>%
arrange(date)
videos = videos %>%
mutate(date = as.Date(publishedAt)) %>%
filter(date > "2018-01-01") %>%
arrange(date)
View(videos)
genstat = data.frame(Channel = "Najwa Shihab", Subscriptios = chstat$statistics$subscriberCount,
Views = chstat$statistics$viewCount,
Videos = chstat$statistics$videoCount, Likes = sum(videostats$likeCount),
Dislikes = sum(videostats$dislikeCount), Comments = sum(videostats$commentCount))
videostats = lapply(as.character(videos$video_id), function(x){
get_stats(video_id = x)
})
View(videostats)
videostats = do.call(rbind.data.frame, videostats)
videostats$title = videos$title
videostats$date = videos$date
videostats = select(videostats, date, title, viewCount, likeCount, dislikeCount, commentCount) %>%
as.tibble() %>%
mutate(viewCount = as.numeric(as.character(viewCount)),
likeCount = as.numeric(as.character(likeCount)),
dislikeCount = as.numeric(as.character(dislikeCount)),
commentCount = as.numeric(as.character(commentCount)))
library("tibble", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
videostats = select(videostats, date, title, viewCount, likeCount, dislikeCount, commentCount) %>%
as.tibble() %>%
mutate(viewCount = as.numeric(as.character(viewCount)),
likeCount = as.numeric(as.character(likeCount)),
dislikeCount = as.numeric(as.character(dislikeCount)),
commentCount = as.numeric(as.character(commentCount)))
videostats = select(videostats, date, title, viewCount, likeCount, dislikeCount, commentCount) %>%
as_tibble() %>%
mutate(viewCount = as.numeric(as.character(viewCount)),
likeCount = as.numeric(as.character(likeCount)),
dislikeCount = as.numeric(as.character(dislikeCount)),
commentCount = as.numeric(as.character(commentCount)))
genstat = data.frame(Channel = "Najwa Shihab", Subscriptios = chstat$statistics$subscriberCount,
Views = chstat$statistics$viewCount,
Videos = chstat$statistics$videoCount, Likes = sum(videostats$likeCount),
Dislikes = sum(videostats$dislikeCount), Comments = sum(videostats$commentCount))
View(videos)
pervideo <- get_all_channel_video_stats(channel_id = "UCo8h2TY_uBkAVUIc14m_KCA", mine = TRUE)
View(pervideo)
?tuber
tuber_GET("https://www.googleapis.com/youtube/analytics/v1/reports","ids=channel%3D%3DMYID&start-date=2016-09-01&end-date=2016-09-30&metrics=views%2Clikes%2CaverageViewDuration&dimensions=day&sort=day&key=MYKEY");
?tuber_GET
libr
library(tuber)
tuber_GET("https://www.googleapis.com/youtube/analytics/v1/reports","ids=channel%3D%3DMYID&start-date=2016-09-01&end-date=2016-09-30&metrics=views%2Clikes%2CaverageViewDuration&dimensions=day&sort=day&key=MYKEY")
install.packages("Rstem", repos="http://www.omegahat.net/R", type="source")
install.packages("SnowballC", repos='http://cran.us.r-project.org')
require(devtools)
install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
require(sentiment)
ls("package:sentiment")
install_url("http://cran.r-project.org/src/contrib/Archive/sentiment/sentiment_0.2.tar.gz")
require(sentiment)
ls("package:sentiment")
library(sentiment)
getwd()
getwd()
setwd("~/Documents/JupyterNotebook/analysis")
getwd()
detikcom <- read.csv(file="detikcom.csv", header = TRUE, sep = ",")
detikcom
View(detikcom)
detikcom_polarity = classify_polarity(detikcom, algorithm = "bayes")
View(detikcom_polarity)
opinion.lexicon.pos = scan("positive.txt", what = "character", comment.char = ";")
opinion.lexicon.neg = scan("negative.txt", what = "character", comment.char = ";")
head()
head(opinion.lexicon.neg)
head(opinion.lexicon.pos)
getSentimentScore = function(sentences, opinion.lexicon.pos, opinion.lexicon.neg, .progress = "none")
getSentimentScore = function(sentences, opinion.lexicon.pos, opinion.lexicon.neg, .progress = "none")
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, opinion.lexicon.pos, opinion.lexicon.neg) {
#remove digit, punctuation, dan special/control character:
sentence = gsub("[[:cntrl:]]", "", gsub("[[:punct:]]", "", gsub("\\d+", "", sentence)))
#convert semua teks menjadi lowercase:
sentence = tolower(sentence)
#pisahkan setiap kalimat menggunakan spasi (space delimiter):
words = unlist(str_split(sentence, "\\s+"))
#lakukan boolean match dari setiap kata-kata menggunakan pos &amp;amp;amp; neg opinion-lexicon:
pos.matches = !is.na(match(words, opinion.lexicon.pos))
neg.matches = !is.na(match(words, opinion.lexicon.neg))
#score sentimen = total positive sentiment - total negative:
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, opinion.lexicon.pos, opinion.lexicon.neg, .progress=.progress)
#return data frame berisi kalimat beserta sentimennya:
return(data.frame(text = sentences, score = scores))
}
#terapkan ke data tweet yang telah kita bersihkan:
detikResult = getSentimentScore(detikcom, opinion.lexicon.pos, opinion.lexicon.neg)
getSentimentScore = function(sentences, opinion.lexicon.pos, opinion.lexicon.neg, .progress = "none")
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, opinion.lexicon.pos, opinion.lexicon.neg) {
#remove digit, punctuation, dan special/control character:
sentence = gsub("[[:cntrl:]]", "", gsub("[[:punct:]]", "", gsub("\\d+", "", sentence)))
#convert semua teks menjadi lowercase:
sentence = tolower(sentence)
#pisahkan setiap kalimat menggunakan spasi (space delimiter):
words = unlist(str_split(sentence, "\\s+"))
#lakukan boolean match dari setiap kata-kata menggunakan pos &amp;amp;amp; neg opinion-lexicon:
pos.matches = !is.na(match(words, opinion.lexicon.pos))
neg.matches = !is.na(match(words, opinion.lexicon.neg))
#score sentimen = total positive sentiment - total negative:
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, opinion.lexicon.pos, opinion.lexicon.neg, .progress=.progress)
#return data frame berisi kalimat beserta sentimennya:
return(data.frame(text = sentences, score = scores))
}
#terapkan ke data tweet yang telah kita bersihkan:
detikResult = getSentimentScore(detikcom, opinion.lexicon.pos, opinion.lexicon.neg)
getSentimentScore = function(sentences, opinion.lexicon.pos, opinion.lexicon.neg, .progress = "none")
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, opinion.lexicon.pos, opinion.lexicon.neg) {
#remove digit, punctuation, dan special/control character:
sentence = gsub("[[:cntrl:]]", "", gsub("[[:punct:]]", "", gsub("\\d+", "", sentence)))
#convert semua teks menjadi lowercase:
sentence = tolower(sentence)
#pisahkan setiap kalimat menggunakan spasi (space delimiter):
words = unlist(str_split(sentence, "\\s+"))
#lakukan boolean match dari setiap kata-kata menggunakan pos &amp;amp;amp; neg opinion-lexicon:
pos.matches = !is.na(match(words, opinion.lexicon.pos))
neg.matches = !is.na(match(words, opinion.lexicon.neg))
#score sentimen = total positive sentiment - total negative:
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, opinion.lexicon.pos, opinion.lexicon.neg, .progress=.progress)
#return data frame berisi kalimat beserta sentimennya:
return(text = sentences, score = scores)
}
#terapkan ke data tweet yang telah kita bersihkan:
detikResult = getSentimentScore(detikcom, opinion.lexicon.pos, opinion.lexicon.neg)
getSentimentScore = function(sentences, opinion.lexicon.pos, opinion.lexicon.neg, .progress = "none")
{
require(plyr)
require(stringr)
scores = laply(sentences, function(sentence, opinion.lexicon.pos, opinion.lexicon.neg) {
#remove digit, punctuation, dan special/control character:
sentence = gsub("[[:cntrl:]]", "", gsub("[[:punct:]]", "", gsub("\\d+", "", sentence)))
#convert semua teks menjadi lowercase:
sentence = tolower(sentence)
#pisahkan setiap kalimat menggunakan spasi (space delimiter):
words = unlist(str_split(sentence, "\\s+"))
#lakukan boolean match dari setiap kata-kata menggunakan pos &amp;amp;amp; neg opinion-lexicon:
pos.matches = !is.na(match(words, opinion.lexicon.pos))
neg.matches = !is.na(match(words, opinion.lexicon.neg))
#score sentimen = total positive sentiment - total negative:
score = sum(pos.matches) - sum(neg.matches)
return(score)
}, opinion.lexicon.pos, opinion.lexicon.neg, .progress=.progress)
#return data frame berisi kalimat beserta sentimennya:
#return(data.frame(text = sentences, score = scores))
}
#terapkan ke data tweet yang telah kita bersihkan:
detikResult = getSentimentScore(detikcom, opinion.lexicon.pos, opinion.lexicon.neg)
head(detikResult)
detikResult
require(plyr)
require(stringr)
detikcom = tolower(detikcom)
View(detikcom)
detikcom <- read.csv(file="detikcom.csv", header = TRUE, sep = ",")
detikcom = tolower(detikcom)
detikcom <- read.csv(file="detikcom.csv", index = FALSE , header = FALSE, sep = ",")
detikcom <- read.csv(file="detikcom.csv", header = FALSE, sep = ",")
View(detikcom)
detikcom <- read.csv(file="detikcom.csv", header = FALSE, sep = " ")
View(detikcom)
detikcom <- read.csv(file="detikcom.csv", header = FALSE, sep = ",")
detikcom <- read.csv(file="detikcom.csv", header = TRUE, sep = ",")
install.packages("udpipe")
vignette("udpipe-tryitout", package = "udpipe")
vignette("udpipe-annotation", package = "udpipe")
vignette("udpipe-usecase-postagging-lemmatisation", package = "udpipe")
# An overview of keyword extraction techniques: https://bnosac.github.io/udpipe/docs/doc7.html
vignette("udpipe-usecase-topicmodelling", package = "udpipe")
vignette("udpipe-train", package = "udpipe")
library(udpipe)
